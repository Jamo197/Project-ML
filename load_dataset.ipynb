{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(batch_size=32, test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Prepare the CIFAR-100 dataset for training and testing with lazy loading.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): The number of samples per batch.\n",
    "        test_size (float): Fraction of data to reserve for testing.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset (tf.data.Dataset): Training dataset generator.\n",
    "        test_dataset (tf.data.Dataset): Testing dataset generator.\n",
    "    \"\"\"\n",
    "    # Load CIFAR-100 data\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "    # Combine full training and test sets for a single split\n",
    "    x_full = tf.concat([x_train_full, x_test], axis=0)\n",
    "    y_full = tf.concat([y_train_full, y_test], axis=0)\n",
    "    \n",
    "    # Split into train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_full, y_full, test_size=test_size, random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Normalization transformation\n",
    "    def normalize(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, label\n",
    "    \n",
    "    # Augmentation transformation for training data\n",
    "    def augment(image, label):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_crop(image, size=[28, 28, 3])  # Optional crop\n",
    "        return normalize(image, label)\n",
    "    \n",
    "    # Create TF Dataset objects with lazy loading\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .shuffle(buffer_size=len(x_train), seed=seed)\n",
    "        .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    test_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "        .map(normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(save_dir='cifar100_dataset'):\n",
    "    \"\"\"\n",
    "    Save the CIFAR-100 dataset locally in a specified directory.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Path to the directory where the dataset will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create directory if it does not exist\n",
    "    os.makedirs(os.path.join(os.path.abspath(os.getcwd()), \"data\", save_dir), exist_ok=True)\n",
    "    \n",
    "    # Load CIFAR-100 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "    \n",
    "    # Save data as numpy arrays\n",
    "    np.save(os.path.join(save_dir, 'x_train.npy'), x_train)\n",
    "    np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(save_dir, 'x_test.npy'), x_test)\n",
    "    np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "    \n",
    "    print(f\"Dataset saved in directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "\u001b[1m 79028224/169001437\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:07:38\u001b[0m 45us/step"
     ]
    }
   ],
   "source": [
    "save_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
